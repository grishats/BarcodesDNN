{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "seed=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        self.cache = torch.zeros(1, 1, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if np.product(x.shape[1:]) != np.product(self.cache.shape[1:]):\n",
    "            self.cache = x\n",
    "            return self.cache\n",
    "\n",
    "        self.cache = self.relu(x + self.cache)\n",
    "        return self.cache\n",
    "        \n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, dataset='CIFAR10', mode='resnet20', width=1, seed=26, variance=None):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        self.Flatten = Flatten()\n",
    "        self.Identity = Identity()\n",
    "        \n",
    "        torch.manual_seed(seed=seed)\n",
    "        \n",
    "        if dataset in ['MNIST', 'FMNIST']:\n",
    "            inp, oup = 1, 10\n",
    "            \n",
    "        elif dataset in ['SVHN', 'CIFAR10']:\n",
    "            inp, oup = 3, 10\n",
    "            \n",
    "        elif dataset in ['CIFAR100']:\n",
    "            inp, oup = 3, 100\n",
    "        \n",
    "        if mode == 'resnet20':\n",
    "            self.net = nn.Sequential(           \n",
    "\n",
    "                 nn.Conv2d(inp, int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU, self.Identity,\n",
    "\n",
    "                 #1.1\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #1.2\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #1.3\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(32 * width), kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #2.1\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #2.2\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #2.3\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(64 * width), kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "\n",
    "                 #3.1\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #3.2\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #3.3\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #Head\n",
    "                 self.GAP,\n",
    "                 self.Flatten,\n",
    "\n",
    "                 nn.Linear(int(64 * width), oup))\n",
    "        \n",
    "\n",
    "        elif mode == 'resnet14':\n",
    "            self.net = nn.Sequential(           \n",
    "\n",
    "                 nn.Conv2d(inp, int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU, self.Identity,\n",
    "\n",
    "                 #1.1\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #1.2\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(32 * width), kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #2.1\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #2.3\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(64 * width), kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "\n",
    "                 #3.1\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #3.3\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "\n",
    "                 #Head\n",
    "                 self.GAP,\n",
    "                 self.Flatten,\n",
    "\n",
    "                 nn.Linear(int(64 * width), oup))\n",
    "\n",
    "            \n",
    "        elif mode == 'resnet9':\n",
    "            self.net = nn.Sequential(           \n",
    "\n",
    "                 nn.Conv2d(inp, int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU, self.Identity,\n",
    "\n",
    "                 #1.1\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(16 * width), int(16 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "                \n",
    "                 nn.MaxPool2d(2, 2),\n",
    "                 nn.Conv2d(int(16 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU, self.Identity, \n",
    "\n",
    "\n",
    "                 #2.1\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(32 * width), int(32 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "                \n",
    "                 nn.MaxPool2d(2, 2),\n",
    "                 nn.Conv2d(int(32 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU, self.Identity,\n",
    "\n",
    "\n",
    "                 #3.1\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "                    \n",
    "                 nn.MaxPool2d(2, 2),\n",
    "                 \n",
    "                #4.1\n",
    "                 self.Identity, #remember\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.ReLU,\n",
    "\n",
    "                 nn.Conv2d(int(64 * width), int(64 * width), kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                 self.Identity,\n",
    "                 nn.MaxPool2d(2, 2),\n",
    "                 \n",
    "                 nn.BatchNorm2d(int(64 * width)),\n",
    "\n",
    "                 #Head\n",
    "                 self.GAP,\n",
    "                 self.Flatten,\n",
    "\n",
    "                 nn.Linear(int(64 * width), oup))\n",
    "\n",
    "            \n",
    "            \n",
    "        for layer in self.net.children():\n",
    "            if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d):\n",
    "                if not isinstance(variance, type(None)):\n",
    "                    nn.init.kaiming_normal_(layer.weight.data, 0.0, variance)\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "                nn.init.constant_(layer.bias.data, 0.01)\n",
    "        \n",
    "    \n",
    "        self.dictify()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        total_parameters = []\n",
    "        for name, par in self.net.named_parameters():\n",
    "            total_parameters.append(par.detach().numpy().reshape(-1, 1))\n",
    "        return np.vstack(total_parameters)\n",
    "\n",
    "    def dictify(self):\n",
    "        self.layer_dict = {}\n",
    "        cum_sum = 0\n",
    "        for index, (name, p) in enumerate(self.net.named_parameters()):\n",
    "            block_size = np.product(p.cpu().detach().numpy().shape)\n",
    "            self.layer_dict[f'{index}_'.zfill(5) + name] = [cum_sum, cum_sum + block_size]\n",
    "            cum_sum += block_size\n",
    "            \n",
    "    def from_vector(self, theta):\n",
    "        pos = 0\n",
    "        for _, p in self.net.named_parameters():\n",
    "            dim = reduce(operator.mul, p.size(), 1)\n",
    "            p.data = torch.tensor(theta[pos:pos+dim], dtype=torch.float32).reshape(p.size())\n",
    "            pos += dim\n",
    "\n",
    "    def set_weights(self, net, theta):\n",
    "        pos = 0\n",
    "        for _, p in net.named_parameters():\n",
    "            dim = reduce(operator.mul, p.size(), 1)\n",
    "            p.data = torch.tensor(\n",
    "                theta[\n",
    "                    pos:pos+dim\n",
    "                ], dtype=torch.float32\n",
    "            ).reshape(p.size())\n",
    "            pos += dim\n",
    "        return net    \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Vectorize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vectorize, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.reshape(input.shape[0], np.product(input.shape[1:]), 1, 1)\n",
    "\n",
    "class MultipleShallowNets(nn.Module):\n",
    "    def __init__(self, nets, kernel_size=3):\n",
    "        super(MultipleShallowNets, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.shallow_net = nets[0]\n",
    "        self.wide_net =[]\n",
    "        self.is_input = True\n",
    "        \n",
    "        self.N = len(nets)\n",
    "        self.n_layers = len(list(self.shallow_net.named_parameters()))\n",
    "        \n",
    "        for index, block in enumerate(self.shallow_net.net):\n",
    "            if isinstance(block, nn.Linear):\n",
    "                block = self.concat_linear(block)\n",
    "                weights_block = [basic_net.net[index] for basic_net in nets]\n",
    "                block = self.concat_layers(weights_block, block)\n",
    "                \n",
    "            \n",
    "            elif isinstance(block, nn.Conv2d):\n",
    "                block = self.concat_convolution(block)\n",
    "                weights_block =[basic_net.net[index] for basic_net in nets]\n",
    "                block = self.concat_layers(weights_block, block)\n",
    "            \n",
    "            elif isinstance(block, Flatten):\n",
    "                block = Vectorize()\n",
    "                \n",
    "            elif isinstance(block, nn.BatchNorm2d):\n",
    "                block = self.concat_batchnorm_weights(block)\n",
    "                weights_block =[basic_net.net[index] for basic_net in nets]\n",
    "                block = self.concat_batchnorm(weights_block, block)\n",
    "            \n",
    "            self.is_input = False\n",
    "            self.wide_net.append(block)\n",
    "        self.net = nn.Sequential(*self.wide_net)\n",
    "        del self.shallow_net    \n",
    "        \n",
    "    def concat_layers(self, layers, wide_layer):\n",
    "        W = torch.cat([layer.weight for layer in layers])\n",
    "        b = torch.cat([layer.bias for layer in layers])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if isinstance(layers[0], nn.Linear):\n",
    "                wide_layer.weight = nn.Parameter(W.reshape(W.shape[0], W.shape[1], 1, 1))\n",
    "            else:\n",
    "                wide_layer.weight = nn.Parameter(W.reshape(W.shape[0], W.shape[1], self.kernel_size, self.kernel_size))\n",
    "                \n",
    "            wide_layer.bias = nn.Parameter(b)\n",
    "\n",
    "        return wide_layer\n",
    "    \n",
    "    def concat_batchnorm(self, layers, wide_layer):\n",
    "        weight, bias, running_mean, running_var = [], [], [], []\n",
    "        \n",
    "        for layer in layers:\n",
    "            layer = layer.state_dict()\n",
    "            \n",
    "            weight.append(layer['weight'])\n",
    "            bias.append(layer['bias'])\n",
    "            running_mean.append(layer['running_mean'])\n",
    "            running_var.append(layer['running_var'])\n",
    "            \n",
    "        weight, bias = torch.cat(weight), torch.cat(bias)  \n",
    "        running_mean, running_var = torch.cat(running_mean), torch.cat(running_var)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            wide_layer.weight = nn.Parameter(weight)\n",
    "            wide_layer.bias = nn.Parameter(bias)\n",
    "            \n",
    "        wide_layer.running_mean = running_mean\n",
    "        wide_layer.running_var = running_var\n",
    "\n",
    "        return wide_layer\n",
    "    \n",
    "    def concat_batchnorm_weights(self, layer):\n",
    "        return nn.BatchNorm2d(int(layer.num_features * self.N))\n",
    "    \n",
    "    def concat_linear(self, layer):\n",
    "        if self.is_input:\n",
    "            return nn.Conv2d(layer.in_features, int(layer.out_features * self.N), kernel_size=1)\n",
    "        else:\n",
    "            return nn.Conv2d(int(layer.in_features * self.N), int(layer.out_features * self.N),\n",
    "                             kernel_size=1, groups=self.N)\n",
    "    \n",
    "    def get_conv_params(self, conv):\n",
    "        parameters = {'in_channels': conv.in_channels, 'out_channels': conv.out_channels,\n",
    "                      'kernel_size': conv.kernel_size[0], 'stride': conv.stride[0],\n",
    "                      'padding': conv.padding[0],'dilation': conv.dilation[0],'groups': conv.groups,\n",
    "                      'bias': (isinstance(conv.bias, type(None)) == False)}\n",
    "        return parameters\n",
    "    \n",
    "    def concat_convolution(self, layer):\n",
    "        parameters = self.get_conv_params(layer)\n",
    "        \n",
    "        if self.is_input:\n",
    "            parameters['out_channels'] = parameters['out_channels'] * self.N\n",
    "            return nn.Conv2d(**parameters)\n",
    "        else:\n",
    "            parameters['out_channels'] = parameters['out_channels'] * self.N\n",
    "            parameters['in_channels'] = parameters['in_channels'] * self.N \n",
    "            parameters['groups'] = self.N\n",
    "            return nn.Conv2d(**parameters)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out.reshape(out.shape[0], out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_net(net, save_gradients=True):\n",
    "    n_models = net.N\n",
    "    n_layers = net.n_layers\n",
    "    networks, gradients = [[] for _ in range(n_models)], [{} for _ in range(n_models)]\n",
    "    \n",
    "    for index, (name, par) in enumerate(net.named_parameters()):\n",
    "        \n",
    "        params = par.cpu().detach().numpy().reshape(n_models, -1)\n",
    "        \n",
    "        if save_gradients:\n",
    "            grad = par.grad.cpu().detach().numpy().reshape(n_models, -1)\n",
    "        \n",
    "        for index in range(n_models):\n",
    "            networks[index].append(params[index].reshape(-1, 1))\n",
    "            \n",
    "            if save_gradients:\n",
    "                gradients[index][str(index) + '_' + name] = grad[index].reshape(-1, 1)\n",
    "            \n",
    "    networks = [np.vstack(net) for net in networks]\n",
    "        \n",
    "    return np.hstack(networks), gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(seed, dataset='FMNIST', batch_size=32):\n",
    "    dataset_dict = {'MNIST': ['/root/LargeScaleExperiment/Data/mnist_data/', datasets.MNIST],\n",
    "                    'FMNIST': ['/root/LargeScaleExperiment/Data/fashion_mnist_data/', datasets.FashionMNIST],\n",
    "                    'CIFAR10': ['/root/LargeScaleExperiment/Data/cifar10_data/', datasets.CIFAR10],\n",
    "                    'SVHN': ['/root/LargeScaleExperiment/Data/svhn_data/', datasets.SVHN],\n",
    "                    'CIFAR100': ['/root/LargeScaleExperiment/Data/cifar100_data/', datasets.CIFAR100]}\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    path, function = dataset_dict[dataset]\n",
    "    \n",
    "    if dataset != 'SVHN':\n",
    "        train_loader = torch.utils.data.DataLoader(function(path, download=False, train=True,\n",
    "                                                   transform=transforms.Compose([transforms.ToTensor()])),\n",
    "                                                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(function(path, download=False, train=False,\n",
    "                                                   transform=transforms.Compose([transforms.ToTensor()])), \n",
    "                                                   batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        train_loader = torch.utils.data.DataLoader(function(path, download=False, split='train',\n",
    "                                                   transform=transforms.Compose([transforms.ToTensor()])),\n",
    "                                                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(function(path, download=False, split='test',\n",
    "                                                   transform=transforms.Compose([transforms.ToTensor()])), \n",
    "                                                   batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def train(net, train_loader, tracker, optimizer, epoch, device):\n",
    "    net.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, label) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False, desc =f'Epoch: {epoch}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net(data.to(device))\n",
    "        \n",
    "        Preds = torch.cat(pred.split(10, dim = 1))\n",
    "        Labels = torch.cat([label for _ in range(net.N)])\n",
    "        \n",
    "        loss = criterion(Preds.to(device), Labels.to(device)) * net.N\n",
    "        loss.backward()\n",
    "        \n",
    "        tracker(net, epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = train_loss/len(train_loader)\n",
    "    tracker.loss_tracker[epoch] = epoch_loss\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate_single(net, train_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, label) in enumerate(train_loader):        \n",
    "        pred = net(data.to(device))\n",
    "        loss = criterion(pred, label.to(device))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(net, train_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    acc = []\n",
    "\n",
    "    for batch_id, (data, label) in enumerate(train_loader):        \n",
    "        pred = net(data.to(device))\n",
    "        \n",
    "        Preds = torch.cat(pred.split(10, dim = 1))\n",
    "        Labels = torch.cat([label for _ in range(net.N)])\n",
    "\n",
    "        acc.append(((Preds.cpu().argmax(dim=1).reshape(-1) == Labels.cpu().reshape(-1))).float().mean().item())\n",
    "        \n",
    "        loss = criterion(Preds.to(device), Labels.to(device)) * net.N\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    return train_loss/len(train_loader), np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_net(net):\n",
    "    n_models = net.N\n",
    "    n_layers = net.n_layers\n",
    "    networks = [[] for _ in range(n_models)]\n",
    "    \n",
    "    for index, (name, par) in enumerate(net.named_parameters()):\n",
    "        \n",
    "        params = par.cpu().detach().numpy().reshape(n_models, -1)\n",
    "        \n",
    "        for index in range(n_models):\n",
    "            networks[index].append(params[index].reshape(-1, 1))\n",
    "            \n",
    "    networks = [np.vstack(net) for net in networks]\n",
    "        \n",
    "    return np.hstack(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scheduler(net, train_loader, optimizer, scheduler, epoch, device, dataset='CIFAR100'):\n",
    "    net.train()\n",
    "        \n",
    "    train_loss = 0\n",
    "    for batch_id, (data, label) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False, desc=f'Epoch: {epoch}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net(data.to(device))\n",
    "        \n",
    "        if dataset == 'CIFAR100':\n",
    "            n_classes=100\n",
    "        else:\n",
    "            n_classes=10\n",
    "        \n",
    "        Preds = torch.cat(pred.split(n_classes, dim = 1))\n",
    "        Labels = torch.cat([label for _ in range(net.N)])\n",
    "        \n",
    "        loss = criterion(Preds.to(device), Labels.to(device)) * net.N\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = scheduler.triangle_scheduler(batch_id, epoch)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = train_loss/len(train_loader)\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def evaluate(net, train_loader, device, dataset='CIFAR100'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    acc = []\n",
    "\n",
    "    for batch_id, (data, label) in enumerate(train_loader):        \n",
    "        pred = net(data.to(device))\n",
    "        \n",
    "        if dataset == 'CIFAR100':\n",
    "            n_classes=100\n",
    "        else:\n",
    "            n_classes=10\n",
    "        \n",
    "        Preds = torch.cat(pred.split(n_classes, dim = 1))\n",
    "        Labels = torch.cat([label for _ in range(net.N)])\n",
    "\n",
    "        acc.append(((Preds.cpu().argmax(dim=1).reshape(-1) == Labels.cpu().reshape(-1))).float().mean().item())\n",
    "        \n",
    "        loss = criterion(Preds.to(device), Labels.to(device)) * net.N\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    return train_loss/len(train_loader), np.mean(acc)\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "class TriangleLR(nn.Module):\n",
    "    def __init__(self, optimizer, epoch_size:int, knots:Tuple[int, int, int],\n",
    "                 values:Tuple[float, float, float], *args, **kwargs):\n",
    "        \n",
    "        self.epoch_size = epoch_size\n",
    "        self.knots = knots\n",
    "        self.values = values\n",
    "\n",
    "        super(TriangleLR, self).__init__()\n",
    "\n",
    "    def triangle_scheduler(self, it:int, epoch:int):\n",
    "        step = epoch + (it / self.epoch_size)\n",
    "\n",
    "        if step > self.knots[-1]:\n",
    "            return self.values[-1]\n",
    "        else:\n",
    "            return np.interp([step], self.knots, self.values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Case1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 0', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss 2.331176830085728 Test loss 2.2580858235265695 Test acc 0.17669209910958422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 1', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 2', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 3', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 4', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 5', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train loss 1.8938416379669403 Test loss 1.8274339893284965 Test acc 0.425085819235035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 6', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 7', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 8', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 9', max=287.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 10', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train loss 1.4959249812136128 Test loss 1.4429262502520692 Test acc 0.5589057797310399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 11', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 12', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 13', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 14', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 15', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train loss 1.192174022621394 Test loss 1.1522153896443985 Test acc 0.6621098103476506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 16', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 17', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 18', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 19', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 20', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train loss 0.9790710908610646 Test loss 0.9786285600241493 Test acc 0.7164304466808543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 21', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 22', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 23', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 24', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 25', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train loss 0.8323155298880999 Test loss 0.837547280624801 Test acc 0.7626326452283299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 26', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 27', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 28', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 29', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 30', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train loss 0.7316996197667271 Test loss 0.7493292011466681 Test acc 0.7840600107230392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 31', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 32', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 33', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 34', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 35', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Train loss 0.6606868522092441 Test loss 0.6872992930459041 Test acc 0.8031351835119958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 36', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 37', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 38', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 39', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 40', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Train loss 0.6079459907908888 Test loss 0.6322909225435818 Test acc 0.8158348477354237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 41', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 42', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e0cbd708784e27b344b5cb7628866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch: 43', max=287.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 8\n",
    "dataset = 'SVHN'\n",
    "device = 'cuda:2'\n",
    "batch_size = 256\n",
    "\n",
    "train_loader, test_loader = make_loaders(26, batch_size=batch_size, dataset=dataset)\n",
    "\n",
    "batches_in_epoch = len(train_loader)\n",
    "\n",
    "epochs = 2001\n",
    "\n",
    "\n",
    "for lr in [1e-4]:\n",
    "    \n",
    "    knots= [0, 1, 2]\n",
    "    vals= [lr, lr, lr]\n",
    "                   \n",
    "    \n",
    "    best_loss_train = 100\n",
    "    \n",
    "    \n",
    "    net = MultipleShallowNets(\n",
    "                              [ResNet(dataset, mode='resnet9', seed=26+index)\n",
    "                               for index in range(N)],\n",
    "                               kernel_size=3).to(device)\n",
    "\n",
    "    logs = 'Case1/'\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0, momentum=0.9, weight_decay=0)\n",
    "    \n",
    "    \n",
    "    tlr = TriangleLR(optimizer, batches_in_epoch, knots, vals)\n",
    "\n",
    "    df = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_scheduler(net, train_loader, optimizer, tlr, epoch, device, dataset=dataset)\n",
    "\n",
    "        if train_loss < best_loss_train:\n",
    "            best_loss_train =train_loss\n",
    "\n",
    "            path = logs + f'ResNet9_{lr}_reg0.torch'\n",
    "            torch.save(net.state_dict(), path)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            test_loss, test_acc = evaluate(net, test_loader, device, dataset=dataset)\n",
    "\n",
    "            print('Epoch', epoch + 1, 'Train loss', train_loss/N, 'Test loss', test_loss/N, 'Test acc', test_acc)\n",
    "\n",
    "            df.append([epoch, train_loss/N, test_loss/N, test_acc])\n",
    "\n",
    "            df_ = pd.DataFrame(df)\n",
    "            df_.columns = ['epoch', 'train_loss', 'test_loss', 'test_acc']\n",
    "\n",
    "            df_.to_csv(f'{logs}ResNet9_{lr}_reg0.csv', index=False)\n",
    "            \n",
    "            if epoch in [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]:\n",
    "                path = logs + f'ResNet9_{lr}_reg0.torch'\n",
    "                torch.save(net.state_dict(), path)\n",
    "\n",
    "    print('!!!!!!!!!!!!NET CONVERGED', 'Seed:', seed, 'Best loss:', best_loss_train/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "N = 8\n",
    "folder = 'Case1'\n",
    "\n",
    "models_path = f'{folder}/Models/'\n",
    "if not os.path.isdir(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "net = MultipleShallowNets([ResNet('SVHN', mode='resnet9', seed=26+index) for index in range(N)], kernel_size=3).to(device)\n",
    "net.load_state_dict(torch.load(f'{folder}/ResNet9_0.0001_reg0.torch', map_location=device))\n",
    "\n",
    "W = factorize_net(net)\n",
    "for i in range(N):\n",
    "    model = ResNet('SVHN', mode='resnet9', seed=26)\n",
    "    model.from_vector(W[:, i])\n",
    "    torch.save(model.state_dict(), models_path + f'model{i}.torch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
